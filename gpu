import os
os.environ['RAY_DEDUP_LOGS'] = '0'

import ray

ray.init()

@ray.remote
class Actor:
    def __init__(self, rank, world_size, rank_dev_map):
        self.rank = rank
        dev = rank_dev_map[rank]
        os.environ['CUDA_VISIBLE_DEVICES'] = str(dev)
        os.environ['RANK'] = str(self.rank)
        os.environ['WORLD_SIZE'] = str(world_size)
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '1025'

    def comm(self):
        import torch
        
        torch.distributed.init_process_group()
        device_tensor = torch.ones(2**30, dtype=torch.uint8, device='cuda')

        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)

        start.record()
        if self.rank == 0:

            torch.distributed.send(device_tensor, 1)
            torch.distributed.recv(device_tensor, 1)
        else:
            torch.distributed.recv(device_tensor, 0)
            torch.distributed.send(device_tensor, 0)
        end.record()
        
        end.synchronize()
        dur_s = start.elapsed_time(end) / 1000
        bytes_comm = 2**30 * 2
        print(f'{dur_s} rate {(bytes_comm/2**30)/dur_s:.02f}')


world_size = 2

rank_dev_map = {
    0: 2,
    1: 3
}

actors = [Actor.remote(i, world_size, rank_dev_map) for i in range(world_size)]

ray.get([a.comm.remote() for a in actors])
