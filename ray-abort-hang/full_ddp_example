#!/usr/bin/env python3

import torch
import torch.nn as nn

import time
import os

import ray
from ray import train
from ray.air import session, Checkpoint
from ray.train.torch import TorchTrainer
from ray.air.config import ScalingConfig
from ray.train.torch.config import TorchConfig, _TorchBackend
from ray.train._internal.worker_group import WorkerGroup

input_size = 1
layer_size = 15
output_size = 1
num_epochs = 3

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, layer_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(layer_size, output_size)

    def forward(self, input):
        return self.layer2(self.relu(self.layer1(input)))

def train_loop_per_worker():
    dataset_shard = session.get_dataset_shard("train")
    model = NeuralNetwork().cuda()
    loss_fn = nn.MSELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

    rank = 0 if '0' in str(train.torch.get_device()) else 1

    model = train.torch.prepare_model(model)

    for epoch in range(num_epochs):
        for batches in dataset_shard.iter_torch_batches(
            batch_size=32, dtypes=torch.float
        ):
            inputs, labels = torch.unsqueeze(batches["x"], 1), batches["y"]
            inputs = inputs.cuda()
            labels = labels.cuda()

            output = model(inputs)
            loss = loss_fn(output, labels)
            optimizer.zero_grad()

            if rank:
                while True:
                    print(f'Rank {rank} sleeping forever before allreduce')
                    time.sleep(5)

            loss.backward()
            optimizer.step()
            print(f"epoch: {epoch}, loss: {loss.item()}")

        session.report(
            {},
            checkpoint=Checkpoint.from_dict(
                dict(epoch=epoch, model=model.state_dict()
            ),
        ))

class CustomConfig(TorchConfig):
    pass

    @property
    def backend_cls(self):
        return CustomBackend

class CustomBackend(_TorchBackend):
    pass
    
    def on_start(self, worker_group: WorkerGroup, backend_config: TorchConfig):

        def set_env_vars():
            print('setting env vars')
            os.environ["NCCL_BLOCKING_WAIT"] = "0"
            os.environ["NCCL_ASYNC_ERROR_HANDLING"] = "1"
            os.environ["TORCH_CPP_LOG_LEVEL"]="INFO"

            os.environ["NCCL_DEBUG"] = "TRACE"

            # This will enable monitor barriers, which is not the behavior we want to test.
            #os.environ["TORCH_DISTRIBUTED_DEBUG"] = "DETAIL"

        worker_group.execute(set_env_vars)

        super(CustomBackend, self).on_start(worker_group, backend_config)

torch_config = CustomConfig(backend='nccl', timeout_s=5)

train_dataset = ray.data.from_items(
    [{"x": x, "y": 2 * x + 1} for x in range(200)]
)
#scaling_config = ScalingConfig(num_workers=3)
# If using GPUs, use the below scaling config instead.
scaling_config = ScalingConfig(num_workers=2, use_gpu=True)
trainer = TorchTrainer(
    train_loop_per_worker=train_loop_per_worker,
    scaling_config=scaling_config,
    torch_config=torch_config,
    datasets={"train": train_dataset})
result = trainer.fit()
