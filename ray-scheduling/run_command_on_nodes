#!/usr/bin/env python3

from typing import Callable
import ray
import os
import sys

from ray.util.placement_group import (
    placement_group,
    placement_group_table,
    remove_placement_group,
)
from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy

ray.init()

@ray.remote(num_cpus=0.01)
class Latch:
    def __init__(self, count):
        self.count = count
        self.original_count = count

    def is_ready(self):
        return self.count == 0

    def count_down(self):
        self.count = max(self.count - 1, 0)

    def reset(self):
        self.count = self.original_count


@ray.remote(num_cpus=0.01)
def waiter(latch, index, shell_command):
    import time

    latch.count_down.remote()
    while not ray.get(latch.is_ready.remote()):
        time.sleep(0.1)

    print(f'running command "{shell_command}"')
    import subprocess

    # shell_command = f'sudo iperf3 -c 172.31.220.209 -p {1025 + index} -O 1 --bytes 5G'
    subprocess.run(shell_command, shell=True, timeout=60)

    import socket

    return socket.gethostname()


def schedule_on_node(node_id, soft=False):
    return ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(
        node_id=node_id, soft=soft
    )


def get_non_gpu_nodes():
    return [node for node in ray.nodes() if node['Resources'].get('GPU') is None]


def wait_for_total_node_count(count, node_generator: Callable = None):
    import time
    while True:
        nodes = ray.nodes() if not node_generator else node_generator()
        if len(nodes) >= count:
            return nodes
        print(f'Waiting for total node count {count}, have {len(nodes)}')
        time.sleep(1)


install_nvme_commands = [
    "sudo apt install xfsprogs -y",
    "sudo mkfs -t xfs /dev/nvme1n1",
    "sudo mkdir /data",
    "sudo mount /dev/nvme1n1 /data",
    "sudo chown -R ray /data",
]

check_disk_speed_commands = [
    "sudo apt install hdparm -y",
    "sudo hdparm -t /dev/nvme1n1",
]

# command = f'sudo iperf3 -s -B {ip_addr} -p {1025 + index}'
# command = f'sudo iperf3 -c 172.31.212.191 -p {1025 + index} --bidir'

def parse_args():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--num-nodes", type=int, required=True)
    args = parser.parse_args()

    return args

def main():
    # commands = ['sudo apt-get install iperf3 -y']
    # commands = ['sudo iperf3 -c 172.31.220.209 -p {1025 + index}']

    args = parse_args()
    print(f'expecting {args.num_nodes}', file=sys.stderr)
    commands = None

    #if os.environ.get('COMMAND_FROM_STDIN', False):
    if True:
        print('Reading commands from stdin')
        commands = [x.strip() for x in sys.stdin.readlines()]
    else:
        commands = install_nvme_commands

    print('Running commands:')
    for command in commands:
        print(f'\t{command}')

    expected_nodes = args.num_nodes
    #node_generator = get_non_gpu_nodes
    node_generator = lambda: ray.nodes()
    node_ids = [node['NodeID'] for node in wait_for_total_node_count(
        count=expected_nodes, node_generator=node_generator)]

    latch = Latch.remote(len(node_ids))
    for command in commands:
        hostnames = ray.get(
            [
                waiter.options(scheduling_strategy=schedule_on_node(node_id)).remote(
                    latch, i, command
                )
                for i, node_id in enumerate(node_ids)
            ]
        )
        ray.get(latch.reset.remote())
        assert len(set(hostnames)) == len(hostnames)


if __name__ == "__main__":
    main()
